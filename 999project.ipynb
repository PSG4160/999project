{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDDBZvJnLVQp"
      },
      "outputs": [],
      "source": [
        "#API 호출 및 데이터 품질 검사\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# .env 파일 로드\n",
        "load_dotenv() #코랩을 사용해서 추후 env형식으로 합쳐보겠습니다.\n",
        "\n",
        "# 환경 변수에서 API 키 불러오기\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# API 호출 함수\n",
        "API_URL = \"https://api.example.com/disaster-info\"\n",
        "\n",
        "def fetch_disaster_data():\n",
        "    params = {\"key\": api_key, \"type\": \"json\", \"count\": 100}\n",
        "    response = requests.get(API_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # 데이터 품질 검사\n",
        "        clean_data = [item for item in data if item.get(\"title\") and item.get(\"description\")]\n",
        "        print(f\"Valid entries: {len(clean_data)} / {len(data)}\")\n",
        "        return clean_data\n",
        "    else:\n",
        "        print(\"Error fetching data:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "# 데이터 저장\n",
        "data = fetch_disaster_data()\n",
        "if data:\n",
        "    with open(\"disaster_data.json\", \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pdf 로드 및 텍스트 추출\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 로드 및 텍스트 추출\n",
        "def load_pdf_with_pypdfloader(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # 문서 리스트를 하나의 텍스트로 합치기\n",
        "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
        "    return documents, full_text\n",
        "\n",
        "# PDF 로드 실행\n",
        "pdf_path = \"\"  # PDF 경로\n",
        "documents, full_text = load_pdf_with_pypdfloader(pdf_path)\n",
        "\n",
        "# 텍스트 추출 확인\n",
        "print(f\"Extracted {len(documents)} pages of content.\")\n",
        "print(full_text[:500])  # 처음 500자 미리보기\n"
      ],
      "metadata": {
        "id": "lfQ8VuHdN6TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트 청킹 및 품질검사\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import json\n",
        "\n",
        "# 텍스트 분할기\n",
        "def chunk_text_with_langchain(full_text, chunk_size=500, overlap=50):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=overlap, separators=[\"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_text(full_text)\n",
        "    return chunks\n",
        "\n",
        "# 텍스트 청킹 실행\n",
        "chunks = chunk_text_with_langchain(full_text)\n",
        "\n",
        "# 청크의 품질 검사: 최소 길이 기준 필터링\n",
        "def quality_check(chunks, min_length=50):\n",
        "    return [chunk for chunk in chunks if len(chunk) >= min_length]\n",
        "\n",
        "filtered_chunks = quality_check(chunks)\n",
        "print(f\"Chunks after quality check: {len(filtered_chunks)} / {len(chunks)}\")\n",
        "\n",
        "# 텍스트 및 청크 저장\n",
        "with open(\"filtered_chunks.json\", \"w\") as f:\n",
        "    json.dump(filtered_chunks, f, indent=4)\n"
      ],
      "metadata": {
        "id": "KsxZXyZTOZpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트 임베딩\n",
        "\n",
        "import os\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# 환경 변수에서 API 키 불러오기\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"API Key not found. Please set OPENAI_API_KEY in your environment.\")\n",
        "\n",
        "# OpenAI 임베딩 모델 초기화\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 로컬 파일 저장소 설정\n",
        "store = LocalFileStore(\"F:\\\\STUDY\\\\sparta\\\\999\\\\박성규\\\\emb\")\n",
        "\n",
        "# 캐시 지원 임베딩 생성\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    underlying_embeddings=embeddings,\n",
        "    document_embedding_cache=store,\n",
        "    namespace=embeddings.model,  # 모델 이름을 네임스페이스로 사용\n",
        ")\n",
        "\n",
        "# 예시 텍스트\n",
        "texts = [\n",
        "    \"지진 발생 시 대처 방법은 무엇인가요?\",\n",
        "    \"화재 발생 시 대피소 안내\",\n",
        "    \"전쟁 상황에서의 시민 대피 방법\"\n",
        "]\n",
        "\n",
        "# 텍스트 임베딩 생성\n",
        "embeddings_result = cached_embedder.embed_documents(texts)\n",
        "\n",
        "# 생성된 임베딩 확인\n",
        "print(\"Generated embeddings:\", embeddings_result)\n",
        "\n",
        "# 캐시된 임베딩을 불러오는 예시\n",
        "cached_result = store.load_data()  # 로컬 저장소에서 불러오기\n",
        "print(\"Cached embeddings:\", cached_result)\n"
      ],
      "metadata": {
        "id": "aTX0WQlROeqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM retrieval\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 유사 문서 검색 함수\n",
        "def find_most_relevant_chunk(question, embeddings, chunks):\n",
        "    question_embedding = model.encode([question])\n",
        "    similarities = cosine_similarity(question_embedding, embeddings)[0]\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "    return chunks[most_similar_index]\n",
        "\n",
        "# 예시 질문\n",
        "question = \"지진 발생 시 대처 방안은?\"\n",
        "relevant_chunk = find_most_relevant_chunk(question, embeddings, filtered_chunks)\n",
        "print(\"Relevant chunk:\", relevant_chunk)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ktxnsj8DUDVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG\n",
        "\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정 (환경 변수에서 불러오기)\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# OpenAI 임베딩 모델 초기화\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai.api_key)\n",
        "\n",
        "# RAG 방식 응답 생성\n",
        "def generate_response_with_rag(question, relevant_text):\n",
        "    system_prompt = (\n",
        "        \"You are an assistant that provides answers based on disaster manuals. \"\n",
        "        \"Use the provided context to answer the question accurately.\"\n",
        "    )\n",
        "\n",
        "    # LLM 호출\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {relevant_text}\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# RAG 기반 응답 생성\n",
        "response = generate_response_with_rag(question, relevant_chunk)\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "P_GE08kzUK_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 질문\n",
        "questions = [\n",
        "    \"가뭄에 대처하는 방법은?\",\n",
        "    \"전쟁 발생 시 시민들이 어떻게 대피해야 하나요?\",\n",
        "    \"화재 발생 시 가장 중요한 행동은 무엇인가요?\"\n",
        "]\n",
        "\n",
        "# 프롬프트 튜닝 및 테스트\n",
        "for q in questions:\n",
        "    relevant_text = find_most_relevant_chunk(q, embeddings, filtered_chunks)\n",
        "    response = generate_response_with_rag(q, relevant_text)\n",
        "    print(f\"Question: {q}\")\n",
        "    print(f\"Answer: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "KHpU2RZgUTPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPgf9AjzjsM2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}